---
title: "Chapter 11"
output: html_notebook
---


```{r setup}
require(tidyverse)
require(rethinking)
require(dagitty)
library(cmdstanr)
```

```{r}
data(chimpanzees)
d <- chimpanzees

d$treatment <- 1 + d$prosoc_left + 2*d$condition

xtabs( ~ treatment + prosoc_left + condition , d )

m11.1 <- quap(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a ,
a ~ dnorm( 0 , 1.5 )
) , data=d )

set.seed(1999)
prior <- extract.prior( m11.1 , n=1e4 )

p <- inv_logit( prior$a )
dens( p , adj=0.1 )

m11.2 <- quap(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a + b[treatment] ,
a ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 10 )
) , data=d )
set.seed(1999)
prior <- extract.prior( m11.2 , n=1e4 )
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )

dens( abs( p[,1] - p[,2] ) , adj=0.1 )

m11.3 <- quap(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a + b[treatment] ,
a ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=d )
set.seed(1999)
prior <- extract.prior( m11.3 , n=1e4 )
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )
mean( abs( p[,1] - p[,2] ) )
dens( abs( p[,1] - p[,2] ) , adj=0.1 )

```

```{r}
# prior trimmed data list 11.10
dat_list <- list(
pulled_left = d$pulled_left,
actor = d$actor,
treatment = as.integer(d$treatment) )

m11.4 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat_list , chains=4 , log_lik=TRUE )
precis( m11.4 , depth=2 )

post <- extract.samples(m11.4)
p_left <- inv_logit( post$a )
plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )

labs <- c("R/N","L/N","R/P","L/P")
plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs )

diffs <- list(
db13 = post$b[,1] - post$b[,3],
db24 = post$b[,2] - post$b[,4] )
plot( precis(diffs) )

pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
pl[1,]

dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link( m11.4 , data=dat )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
```

```{r}
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

dat_list2 <- list(
pulled_left = d$pulled_left,
actor = d$actor,
side = d$side,
cond = d$cond )
m11.5 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + bs[side] + bc[cond] ,
a[actor] ~ dnorm( 0 , 1.5 ),
bs[side] ~ dnorm( 0 , 0.5 ),
bc[cond] ~ dnorm( 0 , 0.5 )
) , data=dat_list2 , chains=4 , log_lik=TRUE )

compare( m11.5 , m11.4 , func=PSIS )

post <- extract.samples(m11.4)
mean( exp(post$b[,4]-post$b[,2]) )
```

```{r}
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
d$pulled_left ,
list( treatment=d$treatment , actor=d$actor ,
side=d$side , cond=d$cond ) ,
sum )
colnames(d_aggregated)[5] <- "left_pulls"
str(d_aggregated)
```

```{r}
dat <- with( d_aggregated , list(
left_pulls = left_pulls,
treatment = treatment,
actor = actor,
side = side,
cond = cond ) )
m11.6 <- ulam(
alist(
left_pulls ~ dbinom( 18 , p ) ,
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ) ,
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat , chains=4 , log_lik=TRUE )

compare( m11.6 , m11.4 , func=PSIS )
```

```{r}
library(rethinking)
data(UCBadmit)
d <- UCBadmit
d

dat_list <- list(
admit = d$admit,
applications = d$applications,
gid = ifelse( d$applicant.gender=="male" , 1 , 2 )
)
m11.7 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] ,
a[gid] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 )
precis( m11.7 , depth=2 )

post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

```{r}
postcheck( m11.7 )
# draw lines connecting points from same dept
for ( i in 1:6 ) {
x <- 1 + 2*(i-1)
y1 <- d$admit[x]/d$applications[x]
y2 <- d$admit[x+1]/d$applications[x+1]
lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}

dat_list$dept_id <- rep(1:6,each=2)
m11.8 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] + delta[dept_id] ,
a[gid] ~ dnorm( 0 , 1.5 ) ,
delta[dept_id] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 , iter=4000 )
precis( m11.8 , depth=2 )

post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )

pg <- with( dat_list , sapply( 1:6 , function(k)
applications[dept_id==k]/sum(applications[dept_id==k]) ) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )

postcheck(m11.8)
pairs(m11.8)
```

```{r}
library(rethinking)
data(Kline)
d <- Kline
d

d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )
curve( dlnorm( x , 3 , .5 ) , from=0 , to=100 , n=200 )
```


```{r}
N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , .2 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )
```

```{r}
x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab="log population" ,
ylab="total tools" )
for ( i in 1:N ) lines( x_seq , lambda[i,] , col=grau() , lwd=1.5 )

plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" ,
ylab="total tools" )
for ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=grau() , lwd=1.5 )
```

```{r}
dat <- list(
T = d$total_tools ,
P = d$P ,
cid = d$contact_id )
# intercept only
m11.9 <- ulam(
alist(
T ~ dpois( lambda ),
log(lambda) <- a,
a ~ dnorm(3,0.5)
), data=dat , chains=4 , log_lik=TRUE )
# interaction model
m11.10 <- ulam(
alist(
  T ~ dpois( lambda ),
log(lambda) <- a[cid] + b[cid]*P,
a[cid] ~ dnorm( 3 , 0.5 ),
b[cid] ~ dnorm( 0 , 0.2 )
), data=dat , chains=4 , log_lik=TRUE )

compare( m11.9 , m11.10 , func=PSIS )
```


```{r}
k <- PSIS( m11.10 , pointwise=TRUE )$k
plot( dat$P , dat$T , xlab="log population (std)" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-1.4 , to=3 , length.out=ns )
# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )
# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
ns <- 100

P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
```


```{r}
num_days <- 30
y <- rpois( num_days , 1.5 )
num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 )

y_all <- c( y , y_new )
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )

# compute the offset
d$log_days <- log( d$days )
# fit the model
m11.12 <- quap(
alist(
y ~ dpois( lambda ),
log(lambda) <- log_days + a + b*monastery,
a ~ dnorm( 0 , 1 ),
b ~ dnorm( 0 , 1 )
), data=d )

post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )

```

```{r}
# simulate career choices among 500 individuals
N <- 500 # number of individuals
income <- c(1,2,5) # expected income of each career
score <- 0.5*income # scores for each career, based on income
# next line converts scores to probabilities
p <- softmax(score[1],score[2],score[3])
# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA,N) # empty vector of choices for each individual
# sample chosen career for each individual
set.seed(34302)
for ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )

code_m11.13 <- "
data{
int N; // number of individuals
int K; // number of possible careers
array[N] int career; // outcome
vector[K] career_income;
}
parameters{
vector[K-1] a; // intercepts
real<lower=0> b; // association of income with choice
}
model{
vector[K] p;
vector[K] s;
a ~ normal( 0 , 1 );
b ~ normal( 0 , 0.5 );
s[1] = a[1] + b*career_income[1];
s[2] = a[2] + b*career_income[2];
s[3] = 0; // pivot
p = softmax( s );
career ~ categorical( p );
}
"

dat_list <- list( N=N , K=3 , career=career , career_income=income )
m11.13 <- stan( model_code=code_m11.13 , data=dat_list , chains=4 )
precis( m11.13 , 2 )

post <- extract.samples( m11.13 )
# set up logit scores

s1 <- with( post , a[,1] + b*income[1] )
s2_orig <- with( post , a[,2] + b*income[2] )
s2_new <- with( post , a[,2] + b*income[2]*2 )
# compute probabilities for original and counterfactual
p_orig <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_orig[i],0) ) )
p_new <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_new[i],0) ) )
# summarize
p_diff <- p_new[2,] - p_orig[2,]
precis( p_diff )
```

```{r}
N <- 500
# simulate family incomes for each individual
family_income <- runif(N)
# assign a unique coefficient for each type of event
b <- c(-2,0,2)
career <- rep(NA,N) # empty vector of choices for each individual
for ( i in 1:N ) {
score <- 0.5*(1:3) + b*family_income[i]
p <- softmax(score[1],score[2],score[3])
career[i] <- sample( 1:3 , size=1 , prob=p )
}
code_m11.14 <- "
data{
int N; // number of observations
int K; // number of outcome values
array[N] int career; // outcome
array[N] real family_income;
}
parameters{
vector[K-1] a; // intercepts
vector[K-1] b; // coefficients on family income
}
model{
vector[K] p;
vector[K] s;
a ~ normal(0,1.5);
b ~ normal(0,1);
for ( i in 1:N ) {
for ( j in 1:(K-1) ) s[j] = a[j] + b[j]*family_income[i];
s[K] = 0; // the pivot
p = softmax( s );
career[i] ~ categorical( p );
}
}
"
dat_list <- list( N=N , K=3 , career=career , family_income=family_income )
m11.14 <- stan( model_code=code_m11.14 , data=dat_list , chains=4 )
precis( m11.14 , 2 )


```

## Practice questions
### 11E1 & 2
```{r}
log_odds <- logit(.35)
odds <- inv_logit(3.2)
```

### 11E3

The exponentiated coefficient describes the relative effect of its associated predictor on the outcome.
```{r}
exp(1.7)

```
So, change of one unit of the predictor increases the odds of the outcome by 547% (?)

### 11E4

POisson regressions require the use of an offset when counts are aggregated over different units of time (or space; i.e. exposure). For example, if two bird observatories report their weekly bird captures but each uses different numbers of mist nets, and instead of understanding how mist nets affect bird counts we want to understand how the landscape around the bird observatory affects bird counts. We have to make sure the counts are comparable by controlling for exposure.

### 11M1

In a disaggregated model, the assumption is that each row of the data is an independent event and therefore there are a ton more ways to see each outcome, which increases the likelihood of each outcome. In an aggregated model, there are fewer ways to see the data, decreasing the likelihood of each outcome. The relative likelihoods should still be the same though?

### 11M2

There is an exponential relationship between predictors and the outcome, so exp(1.7 * predictor). 

### 11M3 & 4
For binomial regression, there is an implied upper limit to the counts, making the logit link appropriate because it scales the predictors to probabilities. For Poisson regression, there is no implied or known upper limit for counts, or they can be infinitely large, so imposing an upper limit is no longer appropriate. The log scales to the order of magnitude but does not impose an upper limit of 1.

### 11M5

Using a logit link for the mean of a Poisson generalized linear model would imply that the expected value and the variance of the distribution are the same, and are between 0 and 1. It also would imply that the relationship of the predictors to the outcome is not exactly exponential, because it constrains the outcome scale.

### 11M6

Binomial distribution has maximum entropy when each trial must result in one of two events and the expected value is constant. The Poisson is the same, because it is just a special case of the binomial distribution where the expected value and the variance are the same.

### 11M7



